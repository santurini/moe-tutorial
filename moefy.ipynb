{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNJQ20B4o5JxuljJ+HKEhaK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Modular Neural Networks - Moefying a ViT\n","\n","Why should we use modular neural networks?\n","\n","It is not always possible to enlarge models without running into hardware limits, just consider the case of edge computing. In these cases, a simple and effective solution to increase the capacity of the model without increasing its computational burden is to use Mixture of Experts.\n","\n","This particular dynamic architecture allows to split a network vertically using an arbitrary number of experts. We refer to this particular typer of Neural Network also as Sparse Neural Networks as only a pre-established number _k_ out of _E_ experts are used for each token based on a routing mechanism implemented through the _gate layer_.\n","\n","## The MoE Layer\n","\n","We'll call $\\{{E_i}\\}$ the set of experts and $p_e$ the routing probabilities, the output of the layer is defined as:\n","\n","$$\n","f(x) = \\sum_{e} p_e E_e(x)\n","$$\n","\n","A simple but effective representation of a MoE layer is the following:\n","\n","<center>\n","<img src=\"https://drive.google.com/uc?id=1HSvgNOHX5W-v5ttTWotpSwWbTddzWrN4\" width=\"600\" height=\"400\">\n","</center>\n","\n","## The Gate Layer\n","\n","<center>\n","<img src=\"https://drive.google.com/uc?id=1LBOykfoMvusGshHSF77Nj0s7xL0VrbfB\" width=\"800\" height=\"300\">\n","</center>\n","\n","To obtain the routing probabilities the most naïve solution consist in using a linear layer that projects the token dimension _N_ to the experts dimension _E_, by applying a *softmax* we obtain for each expert a probability and then we pick for each token the _Top-k_ experts:\n","\n","$$\n","p_e = Top_{k}\\big(softmax(Wx + ϵ)\\big)\n","$$\n","\n","$ϵ$ is Gaussian noise added in order to have a differentiable sampling mechanism as in the Gumbel-Softmax trick.\n","\n","\n"],"metadata":{"id":"OCG0lRhOzm1q"}},{"cell_type":"markdown","source":["# Getting started with FastMoE\n","\n","The extended tutorial to install the library can be found [here](https://github.com/laekov/fastmoe/blob/master/doc/installation-guide.md).\n","\n","There are two options in the installation phase depending on the type of training we are likely to use (distributed or not), in this tutorial we'll consider the latter.\n","\n","In general *FastMoE* allows two different distributed options that can be used independently or jointly (preferred), as one of the advantages of using a MoE is that it can be highly parallelized across different GPUs.\n","\n","<div>\n","  <br>\n","  <img src=\"https://drive.google.com/uc?id=1HzedJ7RqziWK4Z1QASle9bpu1y7Elv6o\" width=\"350\" height=\"230\" hspace=\"30\">\n","\n","  <img src=\"https://drive.google.com/uc?id=1HzedJ7RqziWK4Z1QASle9bpu1y7Elv6o\" width=\"350\" height=\"230\">\n","</div>"],"metadata":{"id":"d9lCfJHnBs-2"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"iwOtLPGRza-Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685633483095,"user_tz":-120,"elapsed":234677,"user":{"displayName":"arturo ghinassi","userId":"02751271912569120406"}},"outputId":"2e5d8902-271e-4797-f1f1-c722f8a19072"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'fastmoe'...\n","remote: Enumerating objects: 2823, done.\u001b[K\n","remote: Counting objects: 100% (796/796), done.\u001b[K\n","remote: Compressing objects: 100% (485/485), done.\u001b[K\n","remote: Total 2823 (delta 325), reused 348 (delta 311), pack-reused 2027\u001b[K\n","Receiving objects: 100% (2823/2823), 964.69 KiB | 18.20 MiB/s, done.\n","Resolving deltas: 100% (1908/1908), done.\n","/content/fastmoe\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ninja\n","  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (0.1.8)\n","Collecting einops\n","  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: ninja, einops\n","Successfully installed einops-0.6.1 ninja-1.11.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Processing /content/fastmoe\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: fastmoe\n","  Building wheel for fastmoe (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fastmoe: filename=fastmoe-1.0.1-cp310-cp310-linux_x86_64.whl size=5398721 sha256=ffe516123450d3fdab9e5045306cb7649e5f28aef4f99b574ca69dce14ea2e05\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-ll1ywhj8/wheels/f5/d8/03/63c5449bf087d070369fc932ba0dcf643c95742ee6bfc16b47\n","Successfully built fastmoe\n","Installing collected packages: fastmoe\n","Successfully installed fastmoe-1.0.1\n"]}],"source":["# clone the repository\n","!git clone https://github.com/laekov/fastmoe.git\n","\n","# move into the folder\n","%cd ./fastmoe\n","\n","# install requirements and utilities\n","!pip install ninja dm-tree einops\n","\n","# install, USE_NCCL set to zero disables the distributed features\n","!USE_NCCL=0 pip install .\n","\n","# try to import\n","import torch\n","import torch.nn as nn\n","import fmoe"]},{"cell_type":"markdown","source":["# Building a FastMoE ViT\n","\n","The code will be based on this repository: https://github.com/lucidrains/vit-pytorch.\n","We are going to use the basic implementation of a ViT.\n","\n","To obtain a MoE we are going to replace the Feed Forward network of the Transformer Block with a mixture of Feed Forward networks, the parameters of the layer are:\n","- **_num_experts_**: the number of experts for each worker (each GPU, in our case just 1)\n","- **_d\\_model_**: the input and output dimension of the layer\n","- **_d\\_hidden_**: the hidden dimension of the first linear layer\n","- **_top_k_**: the number of experts selected for each token\n","- **_gate_**: the type of routing strategy, all the available gates are present here ([gates](https://github.com/laekov/fastmoe/tree/master/fmoe/gates))\n","\n","To switch from a ViT to a MoE ViT we will use the flag _moefy_ and specify the parameters defined above, everything else does not require any modifications.\n","\n","Note that for simplicity, instead of implementing it by hand, FastMoE already has implemented the TransformerMLP and we only need to remember that it gives and **output dimension** equal to the **input_dimension**.\n","```\n","from fmoe.transformer import FMoETransformerMLP\n","\n","# FMoE Transformer MLP\n","self.mlp = FMoETransformerMLP(\n","                  num_expert, \n","                  d_model = dim\n","                  d_hidden = hidden_dim.\n","                  top_k,\n","                  gate\n","                )\n","\n","# if we want output dimension different from input, just add a linear layer\n","\n","self.projection = nn.Linear(dim, output_dim)\n","```"],"metadata":{"id":"o0HOxXIbksJH"}},{"cell_type":"code","source":["#@title MoE FeedForward\n","from fmoe.layers import FMoE\n","from fmoe.linear import FMoELinear\n","from fmoe.gates.gshard_gate import GShardGate\n","\n","class _Expert(nn.Module):\n","    def __init__(self, num_expert, d_model, d_hidden, activation, dropout=0., rank=0):\n","        super().__init__()\n","        self.linear1 = FMoELinear(num_expert, d_model, d_hidden, bias=True, rank=rank)\n","        self.linear2 = FMoELinear(num_expert, d_hidden, d_model, bias=True, rank=rank)\n","        self.dropout = nn.Dropout(dropout)\n","        self.activation = activation\n","\n","    def forward(self, inp, fwd_expert_count):\n","        x = self.linear1(inp, fwd_expert_count)\n","        x = self.activation(x)\n","        x = self.dropout(x)\n","        x = self.linear2(x, fwd_expert_count)\n","        x = self.dropout(x)\n","        return x\n","\n","class FeedForwardMoE(FMoE):\n","    r\"\"\"\n","    A complete MoE MLP module in a Transformer block.\n","    * `activation` is the activation function to be used in MLP in each expert.\n","    * `d_hidden` is the dimension of the MLP layer.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        num_expert=32,\n","        d_model=1024,\n","        d_hidden=4096,\n","        activation=torch.nn.GELU(),\n","        dropout=0.,\n","        expert_dp_comm=\"none\",\n","        expert_rank=0,\n","        **kwargs\n","    ):\n","        def one_expert(d_model):\n","            return _Expert(1, d_model, d_hidden, activation, dropout=dropout, rank=0)\n","        \n","        expert = one_expert\n","        super().__init__(num_expert=num_expert, d_model=d_model, expert=expert, **kwargs)\n","        self.mark_parallel_comm(expert_dp_comm)\n","\n","    def forward(self, inp: torch.Tensor):\n","        r\"\"\"\n","        This module wraps up the FMoE module with reshape, residual and layer\n","        normalization.\n","        \"\"\"\n","        original_shape = inp.shape\n","        inp = inp.reshape(-1, self.d_model)\n","        output = super().forward(inp)\n","        return output.reshape(original_shape)"],"metadata":{"id":"Ielfr4xvLE81","executionInfo":{"status":"ok","timestamp":1685634386874,"user_tz":-120,"elapsed":4,"user":{"displayName":"arturo ghinassi","userId":"02751271912569120406"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["#@title ViT Implementation\n","from einops import rearrange, repeat\n","from einops.layers.torch import Rearrange\n","\n","# helpers\n","\n","def pair(t):\n","    return t if isinstance(t, tuple) else (t, t)\n","\n","# classes\n","\n","class PreNorm(nn.Module):\n","    def __init__(self, dim, fn):\n","        super().__init__()\n","        self.norm = nn.LayerNorm(dim)\n","        self.fn = fn\n","    def forward(self, x, **kwargs):\n","        return self.fn(self.norm(x), **kwargs)\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, dim, hidden_dim, dropout = 0.):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_dim, dim),\n","            nn.Dropout(dropout)\n","        )\n","        \n","    def forward(self, x):\n","        return self.net(x)\n","\n","class Attention(nn.Module):\n","    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n","        super().__init__()\n","        inner_dim = dim_head *  heads\n","        project_out = not (heads == 1 and dim_head == dim)\n","\n","        self.heads = heads\n","        self.scale = dim_head ** -0.5\n","\n","        self.attend = nn.Softmax(dim = -1)\n","        self.dropout = nn.Dropout(dropout)\n","\n","        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n","\n","        self.to_out = nn.Sequential(\n","            nn.Linear(inner_dim, dim),\n","            nn.Dropout(dropout)\n","        ) if project_out else nn.Identity()\n","\n","    def forward(self, x):\n","        qkv = self.to_qkv(x).chunk(3, dim = -1)\n","        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n","\n","        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n","\n","        attn = self.attend(dots)\n","        attn = self.dropout(attn)\n","\n","        out = torch.matmul(attn, v)\n","        out = rearrange(out, 'b h n d -> b n (h d)')\n","        return self.to_out(out)\n","\n","class Transformer(nn.Module):\n","    def __init__(\n","        self, \n","        dim, \n","        depth, \n","        heads, \n","        dim_head,\n","        mlp_dim,\n","        dropout = 0.,\n","        moefy = False,\n","        num_expert = None,\n","        top_k = None,\n","        gate = None\n","        ):\n","      \n","        super().__init__()\n","        if moefy:\n","          assert num_expert and top_k and gate, \\\n","          \"If 'moefy' is set to True but none of the following arguments should be None: num_expert={}, num_gpus={}, top_k={}, gate={}\" \\\n","          .format(num_expert, top_k, gate)\n","          \n","        self.layers = nn.ModuleList([])\n","        for _ in range(depth):\n","            self.layers.append(nn.ModuleList([\n","                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n","                PreNorm(dim, FeedForwardMoE(num_expert, dim, mlp_dim, dropout=dropout, top_k=top_k, gate=GShardGate) \n","                if moefy else FeedForward(dim, mlp_dim, dropout = dropout))\n","            ]))\n","            \n","    def forward(self, x):\n","        for attn, ff in self.layers:\n","            x = attn(x) + x\n","            x = ff(x) + x\n","        return x\n","\n","class ViT(nn.Module):\n","    def __init__(\n","        self, *, \n","        image_size, \n","        patch_size, \n","        num_classes, \n","        dim, depth, \n","        heads, \n","        mlp_dim, \n","        pool = 'cls', \n","        channels = 3, \n","        dim_head = 64, \n","        dropout = 0., \n","        emb_dropout = 0.,\n","        moefy = False,\n","        num_expert = None,\n","        top_k = None,\n","        gate = None\n","        ):\n","      \n","        super().__init__()\n","        image_height, image_width = pair(image_size)\n","        patch_height, patch_width = pair(patch_size)\n","\n","        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n","\n","        num_patches = (image_height // patch_height) * (image_width // patch_width)\n","        patch_dim = channels * patch_height * patch_width\n","        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n","\n","        self.to_patch_embedding = nn.Sequential(\n","            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n","            nn.LayerNorm(patch_dim),\n","            nn.Linear(patch_dim, dim),\n","            nn.LayerNorm(dim),\n","        )\n","\n","        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n","        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n","        self.dropout = nn.Dropout(emb_dropout)\n","\n","        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout, moefy, num_expert, top_k, gate)\n","\n","        self.pool = pool\n","        self.to_latent = nn.Identity()\n","\n","        self.mlp_head = nn.Sequential(\n","            nn.LayerNorm(dim),\n","            nn.Linear(dim, num_classes)\n","        )\n","\n","    def forward(self, img):\n","        x = self.to_patch_embedding(img)\n","        b, n, _ = x.shape\n","\n","        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n","        x = torch.cat((cls_tokens, x), dim=1)\n","        x += self.pos_embedding[:, :(n + 1)]\n","        x = self.dropout(x)\n","\n","        x = self.transformer(x)\n","\n","        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n","\n","        x = self.to_latent(x)\n","        return self.mlp_head(x)"],"metadata":{"id":"l83eyv2nkro_","executionInfo":{"status":"ok","timestamp":1685634634382,"user_tz":-120,"elapsed":242,"user":{"displayName":"arturo ghinassi","userId":"02751271912569120406"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["# Number of parameters comparison"],"metadata":{"id":"OFeMCBNfLmZv"}},{"cell_type":"code","source":["#@title ViT\n","vit = ViT(\n","    image_size=32,\n","    patch_size=4,\n","    num_classes=10,\n","    dim=32,\n","    depth=2,\n","    heads=2,\n","    mlp_dim=32\n","    ).cuda()\n","\n","print(\"Number of parameters in standard ViT:\", sum(p.numel() for p in vit.parameters() if p.requires_grad))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"yMRA0qUS8q35","executionInfo":{"status":"ok","timestamp":1685634637037,"user_tz":-120,"elapsed":5,"user":{"displayName":"arturo ghinassi","userId":"02751271912569120406"}},"outputId":"752a5d01-ae07-4bd0-fc36-60613af7fb87"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of parameters in standard ViT: 41546\n"]}]},{"cell_type":"code","source":["#@title MoE ViT\n","moe_vit = ViT(\n","    image_size=32,\n","    patch_size=4,\n","    num_classes=10,\n","    dim=32,\n","    depth=2,\n","    heads=2,\n","    mlp_dim=32,\n","    moefy = True,\n","    num_expert = 4,\n","    top_k = 2,\n","    gate = GShardGate\n","    ).cuda()\n","\n","print(\"Number of parameters in MoE ViT:\", sum(p.numel() for p in moe_vit.parameters() if p.requires_grad))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"grvBoooOF9yT","executionInfo":{"status":"ok","timestamp":1685634645429,"user_tz":-120,"elapsed":252,"user":{"displayName":"arturo ghinassi","userId":"02751271912569120406"}},"outputId":"4a52cba4-2b9e-4d82-c999-97d77a5a0392"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of parameters in MoE ViT: 54482\n"]}]}]}